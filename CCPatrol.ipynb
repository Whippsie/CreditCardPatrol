{
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "cells": [
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "822c18801d798c6491fe2017da165deb7456e06d",
        "_execution_state": "idle"
      },
      "source": "",
      "execution_count": null,
      "cell_type": "markdown",
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "dd7851674c1134ec916e56a1eb302ad297c9f84e",
        "_execution_state": "idle"
      },
      "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.preprocessing import StandardScaler # for preprocessing the data\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVC # SVM\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\n#from sklearn.cross_validation import train_test_split # to split the data\n#from sklearn.cross_validation import KFold # For cross vbalidation\nfrom sklearn.model_selection import GridSearchCV # for tunnig hyper parameter it will use all combination of given parameters\nfrom sklearn.model_selection import RandomizedSearchCV # same for tunning hyper parameter but will use random combinations of parameters\nfrom sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n\n%matplotlib inline",
      "execution_count": 19,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "creditcard.csv\n\n"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "de2bb47c7032d4c409acca9e1f45f26a5305e4f3",
        "_execution_state": "idle"
      },
      "source": "data = pd.read_csv(\"../input/creditcard.csv\",header = 0)\ndata.head()",
      "execution_count": 20,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...         V21       V22       V23       V24  \\\n0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n\n        V25       V26       V27       V28  Amount  Class  \n0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]",
            "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "ffd654eccd998f4384cb56dce34ebbf27aacbb17",
        "_execution_state": "idle"
      },
      "source": "## Pre-processing the data\n## Normalizing the amount column\n\nfrom sklearn.preprocessing import StandardScaler\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\ndata = data.drop(['Time','Amount'],axis=1)\ndata.head()",
      "execution_count": 21,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "         V1        V2        V3        V4        V5        V6        V7  \\\n0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9       V10     ...           V21       V22       V23  \\\n0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n\n        V24       V25       V26       V27       V28  Class  normAmount  \n0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n\n[5 rows x 30 columns]",
            "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Class</th>\n      <th>normAmount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>0.090794</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>0</td>\n      <td>0.244964</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>-0.166974</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>0</td>\n      <td>-0.342475</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>0.207643</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>0</td>\n      <td>1.160686</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>-0.054952</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>0</td>\n      <td>0.140534</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>0.753074</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>0</td>\n      <td>-0.073403</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "24d39b64a751ee237c7c0ce371906503e1b8c020",
        "_execution_state": "idle"
      },
      "source": "## Since the data is largely imbalanced we need to resample the data such that the proportion/ratio between fraudulent and normal transactions are relativeley similar.\n\nx = data.loc[:, data.columns != 'Class']\ny = data.loc[:, data.columns == 'Class']",
      "execution_count": 22,
      "cell_type": "code",
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "8dd0e872c2e26996cccc20d4f3ab101335c37321",
        "_execution_state": "idle"
      },
      "source": "# Number of fraudelent transaction in the existing data\nnumberOffraudulentTransaction = len(data[data.Class == 1])\nfraudIndices = np.array(data[data.Class == 1].index)\n\n# Picking the indices of the normal classes\nnormalIndices = data[data.Class == 0].index\n\n# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\nrandom_normal_indices = np.random.choice(normalIndices, numberOffraudulentTransaction, replace = False)\nrandom_normal_indices = np.array(random_normal_indices)\n\n# Appending the 2 indices\nunder_sample_indices = np.concatenate([fraudIndices,random_normal_indices])\n\n# Under sample dataset\nunder_sample_data = data.iloc[under_sample_indices,:]\n\nX_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\ny_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']\n\n# Showing ratio\nprint(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\nprint(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\nprint(\"Total number of transactions in resampled data: \", len(under_sample_data))",
      "execution_count": 23,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Percentage of normal transactions:  0.5\nPercentage of fraud transactions:  0.5\nTotal number of transactions in resampled data:  984\n"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "4676135f2593511796b62e34333ef531c5f53771",
        "_execution_state": "idle"
      },
      "source": "#SVM\nsvc = SVC(C=1, kernel='linear')\nsvc2 = SVC(C=1, kernel='polynomial')\nsvc3 = SVC(C=1, kernel='rbf')\nsvc4 = SVC(C=1, kernel='sigmoid')\n\n#RF, clf = classifier\nclf = RandomForestClassifier(n_estimators=100, n_jobs=2, min_samples_split=2, random_state=0)\n#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n#scores = cross_val_score(clf, X, y)\n#scores.mean()    \n\n#clf.fit (x,y)...\n\nclf2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\n#scores = cross_val_score(clf, X, y)\n#scores.mean()                             \n\nclf3 = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n#scores = cross_val_score(clf, X, y)\n#scores.mean() > 0.999\n\n'''The main parameters to adjust when using these methods is n_estimators and max_features. \nThe larger n_estimators the better, but also the longer it  will take to compute. \nmax_feat is the size of the random subsets of features to consider when splitting a node. \nlower = greater the reduction of variance, but also the greater the increase in bias. \nEmpirical good default values are max_features=n_features for regression problems, \nand max_features=sqrt(n_features) for classification tasks (where n_features is the number of features \nin the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=1 \n(i.e., when fully developing the trees). The best parameter values should always be cross-validated. \nIn addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) \nwhile the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using \nbootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. \nThis can be enabled by setting oob_score=True.'''",
      "execution_count": 24,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'The main parameters to adjust when using these methods is n_estimators and max_features. \\nThe larger n_estimators the better, but also the longer it  will take to compute. \\nmax_feat is the size of the random subsets of features to consider when splitting a node. \\nlower = greater the reduction of variance, but also the greater the increase in bias. \\nEmpirical good default values are max_features=n_features for regression problems, \\nand max_features=sqrt(n_features) for classification tasks (where n_features is the number of features \\nin the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=1 \\n(i.e., when fully developing the trees). The best parameter values should always be cross-validated. \\nIn addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) \\nwhile the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using \\nbootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. \\nThis can be enabled by setting oob_score=True.'"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}