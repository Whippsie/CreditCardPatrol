{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4306c3e3-093a-4316-a144-e07ee32079e9",
    "_execution_state": "idle",
    "_uuid": "822c18801d798c6491fe2017da165deb7456e06d",
    "collapsed": false
   },
   "source": [
    "Hello guys we are a team of 3 girls learning to do machine learning. We have put this notebook public for insight on how to improve our models. Please know that the models we used are taken from other kaggle models in this thread. Thus, some parts are strongly inspired from these other models. We are now trying to twerk the code to make it our own and learn how we can improve these models. Big thanks to :\n",
    "\n",
    "[Data preprocessing && resampling][1]\n",
    "\n",
    "[Over+Under+SMOTE][2]\n",
    "\n",
    "[Model train + why not ROC][3]\n",
    "\n",
    "  [1]: https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now\n",
    "  [2]: https://www.kaggle.com/gargmanish/how-to-handle-imbalance-data-study-in-detail\n",
    "  [3]: https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "66d8dc05-d7da-4cf9-8021-5a4a3505da33",
    "_execution_state": "idle",
    "_uuid": "dd7851674c1134ec916e56a1eb302ad297c9f84e",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC # SVM\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score,confusion_matrix,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV # for tunnig hyper parameter it will use all combination of given parameters\n",
    "from sklearn.model_selection import RandomizedSearchCV # same for tunning hyper parameter but will use random combinations of parameters\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "d9b7d5fe-8192-4c5a-8b24-da0147140b74",
    "_execution_state": "idle",
    "_uuid": "de2bb47c7032d4c409acca9e1f45f26a5305e4f3",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/zahrakhambaty/Downloads/creditcard.csv\",header = 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "42130d29-9f0d-4248-bdb9-1514fcf2d151",
    "_execution_state": "idle",
    "_uuid": "ffd654eccd998f4384cb56dce34ebbf27aacbb17",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...           V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pre-processing the data\n",
    "## Normalizing the amount column\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "d2405d3e-977a-4539-be51-aae0e4189a1d",
    "_execution_state": "idle",
    "_uuid": "24d39b64a751ee237c7c0ce371906503e1b8c020",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Since the data is largely imbalanced we need to resample the data such that the proportion/ratio between fraudulent and normal transactions are relativeley similar.\n",
    "\n",
    "x = data.loc[:, data.columns != 'Class']\n",
    "y = data.loc[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "1b49cbe6-8f76-48ca-8789-1ed231fcf18c",
    "_execution_state": "idle",
    "_uuid": "8dd0e872c2e26996cccc20d4f3ab101335c37321",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of normal transactions:  0.5\n",
      "Percentage of fraud transactions:  0.5\n",
      "Total number of transactions in resampled data:  984\n"
     ]
    }
   ],
   "source": [
    "#UNDERSAMPLING\n",
    "# Number of fraudelent transaction in the existing data\n",
    "numberOffraudulentTransaction = len(data[data.Class == 1])\n",
    "fraudIndices = np.array(data[data.Class == 1].index)\n",
    "\n",
    "# Picking the indices of the normal classes\n",
    "normalIndices = data[data.Class == 0].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(normalIndices, numberOffraudulentTransaction, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraudIndices,random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sample_data = data.iloc[under_sample_indices,:]\n",
    "\n",
    "x_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\n",
    "y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']\n",
    "\n",
    "# Showing ratio\n",
    "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
    "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
    "print(\"Total number of transactions in resampled data: \", len(under_sample_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "c08a5b77-f4b6-4d0f-856a-6b4652cf2ce9",
    "_execution_state": "idle",
    "_uuid": "d07eca2a35bbe3027e1d2576965d95f9f16f8efd",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: length of normal data 159900\n",
      "train data: length of fraud data 303\n",
      "For Validation Set :length of normal data 53320\n",
      "For Validation Set :length of fraud data 82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Splitting the data into Training,Validation and Test Set##\n",
    "## Test Set needs to be unused till the mere end##\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data,y, test_size=0.25, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size=0.25, random_state=42)\n",
    "# #Figuring out the ratio of normal transction and fraudelent transaction from training data# #\n",
    "normal_tdata = X_train[X_train[\"Class\"]==0]\n",
    "print(\"train data: length of normal data\",len(normal_tdata))\n",
    "fraud_tdata = X_train[X_train[\"Class\"]==1]\n",
    "print(\"train data: length of fraud data\",len(fraud_tdata))\n",
    "## dataset for validation set ##\n",
    "normal_vdata = X_val[X_val[\"Class\"]==0]\n",
    "print(\"For Validation Set :length of normal data\",len(normal_vdata))\n",
    "fraud_vdata = X_val[X_val[\"Class\"]==1]\n",
    "print(\"For Validation Set :length of fraud data\",len(fraud_vdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "169e2a09-6f94-4057-b1a4-6fae855d94c9",
    "_execution_state": "idle",
    "_uuid": "b927676a28fb48050cb6d63d0382774618b2075d",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SMOTE #OVERSAMPLING\n",
    "#Since the data is highly imbalanced we use the imblearn to balance out the data by introducing more fraudulent data ##\n",
    "#basically oversampling of data but with synthesized data so no duplicates\n",
    "sm = SMOTE(random_state=12, ratio = 0.6, k_neighbors=5)\n",
    "x_train_res, y_train_res = sm.fit_sample(X_train, Y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "15038d12-40e0-4e32-b6f7-169ada0b0164",
    "_execution_state": "idle",
    "_uuid": "4bd0bd557bc3e7f8713e13ab981a8d5e2ab11309",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data is  255840\n",
      "Number of normal transcation in oversampled data 95940\n",
      "No.of fraud transcation 159900\n",
      "Proportion of Normal data in oversampled data is  0.625\n",
      "Proportion of fraud data in oversampled data is  0.375\n"
     ]
    }
   ],
   "source": [
    "a = x_train_res[:,28]\n",
    "b= np.count_nonzero(a == 1)\n",
    "c= np.count_nonzero(a == 0)\n",
    "print(\"length of oversampled data is \",len(x_train_res))\n",
    "print(\"Number of normal transcation in oversampled data\",b)\n",
    "print(\"No.of fraud transcation\",c)\n",
    "print(\"Proportion of Normal data in oversampled data is \",c/len(x_train_res))\n",
    "print(\"Proportion of fraud data in oversampled data is \",b/len(x_train_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "21214f06-a021-46a4-bbfb-132e020c0f1e",
    "_execution_state": "idle",
    "_uuid": "df61e2342b27cf77acd8652f0ac0543010572e19",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNDERSAMPLING\n"
     ]
    }
   ],
   "source": [
    "print (\"UNDERSAMPLING\")\n",
    "df = under_sample_data\n",
    "#train, validate, test = np.split(df.sample(frac=1), [int(.5*len(df)), int(.75*len(df))])\n",
    "x, x_test, y, y_test = train_test_split(x_undersample,y_undersample,test_size=0.25,train_size=0.75)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.33,train_size =0.66)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Get mean+average and standardize to Z\n",
    "x_train = scaler.fit_transform (x_train)\n",
    "\n",
    "#Apply same transformation to hidden data\n",
    "x_cv = scaler.transform (x_cv)\n",
    "x_test = scaler.transform (x_test)\n",
    "\n",
    "# cross-validate needs to be here (after the splitting for proper X-V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "6d6a5d38-7a63-4112-a5f8-bfc8d9467bd7",
    "_execution_state": "idle",
    "_uuid": "4676135f2593511796b62e34333ef531c5f53771",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.959349593496\n",
      "[[127   3]\n",
      " [  7 109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       130\n",
      "          1       0.97      0.94      0.96       116\n",
      "\n",
      "avg / total       0.96      0.96      0.96       246\n",
      "\n",
      "LOGISTIC REGRESSION CLASS WEIGHT BALANCED\n",
      "SVM\n",
      "[[127   3]\n",
      " [ 10 106]]\n",
      "0.942222222222\n",
      "RANDOMFOREST\n",
      "[[126   4]\n",
      " [  9 107]]\n",
      "0.942731277533\n",
      "RANDOMFOREST3\n",
      "[[126   4]\n",
      " [  9 107]]\n",
      "0.942731277533\n",
      "RANDOMFOREST4\n",
      "[[126   4]\n",
      " [  9 107]]\n",
      "0.942731277533\n",
      "DECISIONTREE1\n",
      "[[119  11]\n",
      " [ 10 106]]\n",
      "0.909871244635\n",
      "DECISIONTREE2\n",
      "[[121   9]\n",
      " [ 10 106]]\n",
      "0.917748917749\n",
      "LOGISTIC REGRESSION PAREMETER 1\n",
      "0.959349593496\n",
      "[[127   3]\n",
      " [  7 109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       130\n",
      "          1       0.97      0.94      0.96       116\n",
      "\n",
      "avg / total       0.96      0.96      0.96       246\n",
      "\n",
      "LOGISTIC REGRESSION PAREMETER 50\n",
      "0.959349593496\n",
      "[[125   5]\n",
      " [  5 111]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96       130\n",
      "          1       0.96      0.96      0.96       116\n",
      "\n",
      "avg / total       0.96      0.96      0.96       246\n",
      "\n",
      "LOGISTIC REGRESSION PAREMETER 500\n",
      "0.959349593496\n",
      "[[124   6]\n",
      " [  4 112]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96       130\n",
      "          1       0.95      0.97      0.96       116\n",
      "\n",
      "avg / total       0.96      0.96      0.96       246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#UNDERSAMPLING\n",
    "# My logic is regressing, guys!\n",
    "logi = LogisticRegression(class_weight='balanced')\n",
    "mdl = logi.fit(x_train, y_train.values.ravel())\n",
    "predictions = logi.predict(x_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))\n",
    "print (classification_report(y_test, predictions))\n",
    "print(\"LOGISTIC REGRESSION CLASS WEIGHT BALANCED\")\n",
    "\n",
    "\n",
    "# Support vector machine, boss!\n",
    "print(\"SVM\")\n",
    "\n",
    "#Other models doing 75%\n",
    "svc = SVC(C=1, kernel='linear')\n",
    "svc.fit (x_train,y_train.values.ravel())\n",
    "ypredsvc = svc.predict (x_test)\n",
    "print(confusion_matrix(y_test, ypredsvc))\n",
    "print(f1_score(y_test, ypredsvc))\n",
    "\n",
    "\n",
    "# Random Forest stories, mate!\n",
    "\n",
    "classif = RandomForestClassifier(n_estimators=100, n_jobs=2, min_samples_split=2, random_state=0)\n",
    "#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n",
    "#scores = cross_val_score(clf, X, y)\n",
    "#scores.mean()    \n",
    "classif.fit(x_train, y_train.values.ravel())\n",
    "y_pred_test_rf = classif.predict(x_test)\n",
    "print(\"RANDOMFOREST\")\n",
    "print(confusion_matrix(y_test, y_pred_test_rf))\n",
    "print(f1_score(y_test, y_pred_test_rf))\n",
    "\n",
    "\n",
    "classif = RandomForestClassifier(n_estimators=100, n_jobs=2, min_samples_split=2, random_state=42)\n",
    "#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n",
    "#scores = cross_val_score(clf, X, y)\n",
    "#scores.mean()    \n",
    "classif.fit(x_train, y_train.values.ravel())\n",
    "y_pred_test_rf = classif.predict(x_test)\n",
    "print(\"RANDOMFOREST3\")\n",
    "print(confusion_matrix(y_test, y_pred_test_rf))\n",
    "print(f1_score(y_test, y_pred_test_rf))\n",
    "\n",
    "classif = RandomForestClassifier(n_estimators=100, n_jobs=5, min_samples_split=2, random_state=0)\n",
    "#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n",
    "#scores = cross_val_score(clf, X, y)\n",
    "#scores.mean()    \n",
    "classif.fit(x_train, y_train.values.ravel())\n",
    "y_pred_test_rf = classif.predict(x_test)\n",
    "print(\"RANDOMFOREST4\")\n",
    "print(confusion_matrix(y_test, y_pred_test_rf))\n",
    "print(f1_score(y_test, y_pred_test_rf))\n",
    "\n",
    "\n",
    "# Decision Tree, baby!\n",
    "classif2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\n",
    "classif2.fit(x_train, y_train.values.ravel())\n",
    "y_pred_test_clf2 = classif2.predict(x_test)\n",
    "#scores = cross_val_score(clf, x_train, y_train)\n",
    "#scores.mean()\n",
    "print(\"DECISIONTREE1\")\n",
    "print(confusion_matrix(y_test, y_pred_test_clf2))\n",
    "print(f1_score(y_test, y_pred_test_clf2))\n",
    "\n",
    "classif2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=15)\n",
    "classif2.fit(x_train, y_train.values.ravel())\n",
    "y_pred_test_clf2 = classif2.predict(x_test)\n",
    "#scores = cross_val_score(clf, x_train, y_train)\n",
    "#scores.mean()\n",
    "print(\"DECISIONTREE2\")\n",
    "print(confusion_matrix(y_test, y_pred_test_clf2))\n",
    "print(f1_score(y_test, y_pred_test_clf2))\n",
    "\n",
    "\n",
    "# LOGISTIC REGRESSION,TUNING THE PARAMETERS TO SEE THE RESULTS, captain!\n",
    "lr = LogisticRegression(C = 1, penalty = 'l1')\n",
    "lr.fit(x_train, y_train.values.ravel())\n",
    "y_pred_test_nn = lr.predict(x_test)\n",
    "print(\"LOGISTIC REGRESSION PAREMETER 1\")\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))\n",
    "print (classification_report(y_test, predictions))\n",
    "\n",
    "lr = LogisticRegression(C = 50, penalty = 'l1')\n",
    "lr.fit(x_train, y_train.values.ravel())\n",
    "y_pred_test_nn = lr.predict(x_test)\n",
    "print(\"LOGISTIC REGRESSION PAREMETER 50\")\n",
    "print(accuracy_score(y_test, y_pred_test_nn))\n",
    "print (confusion_matrix(y_test, y_pred_test_nn))\n",
    "print (classification_report(y_test, y_pred_test_nn))\n",
    "\n",
    "#l1 and l2 gives very similar result\n",
    "lr = LogisticRegression(C = 500, penalty = 'l1')\n",
    "lr.fit(x_train, y_train.values.ravel())\n",
    "y_pred_test_nn = lr.predict(x_test)\n",
    "print(\"LOGISTIC REGRESSION PAREMETER 500\")\n",
    "print(accuracy_score(y_test, y_pred_test_nn))\n",
    "print (confusion_matrix(y_test, y_pred_test_nn))\n",
    "print (classification_report(y_test, y_pred_test_nn))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "2f12ffbe-947e-43d9-b4e8-c2a20e4f5d63",
    "_execution_state": "busy",
    "_uuid": "55244e4b10b594c7bfeb8a381e25cdd7bde9b527",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERSAMPLING\n",
      "1.0\n",
      "[[71095     0]\n",
      " [    0   107]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     71095\n",
      "          1       1.00      1.00      1.00       107\n",
      "\n",
      "avg / total       1.00      1.00      1.00     71202\n",
      "\n",
      "LOGICREGRESSION WITH Balanced weight \n",
      "LOGICREGRESSION WITH C=5 \n",
      "[[71095     0]\n",
      " [    0   107]]\n",
      "1.0\n",
      "RANDOMFOREST\n",
      "[[71095     0]\n",
      " [    0   107]]\n",
      "1.0\n",
      "DECISIONTREE\n",
      "[[71095     0]\n",
      " [    0   107]]\n",
      "1.0\n",
      "Neural Network\n",
      "[[  9.99991461e-01   8.53883196e-06]\n",
      " [  9.99991461e-01   8.53883196e-06]\n",
      " [  9.99991461e-01   8.53883196e-06]\n",
      " ..., \n",
      " [  9.99991461e-01   8.53883196e-06]\n",
      " [  9.99991461e-01   8.53883196e-06]\n",
      " [  9.99991461e-01   8.53883196e-06]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"OVERSAMPLING\")\n",
    "\n",
    "#substract the min of the column divide by the max  for the whole column\n",
    "#also apply on test+validation\n",
    "#try without normalizing at all, nor stabilize\n",
    "\n",
    "# LOGICREGRESSION, captain!\n",
    "\n",
    "logi = LogisticRegression(class_weight='balanced')\n",
    "mdl = logi.fit(x_train_res, y_train_res)\n",
    "predictions2 = logi.predict(X_test)\n",
    "print(accuracy_score(Y_test, predictions2))\n",
    "print (confusion_matrix(Y_test, predictions2))\n",
    "print (classification_report(Y_test, predictions2))\n",
    "print(\"LOGICREGRESSION WITH Balanced weight \")\n",
    "\n",
    "# LOGICREGRESSION, captain!\n",
    "lr = LogisticRegression(C = 5, penalty = 'l1')\n",
    "lr.fit(x_train_res,y_train_res)\n",
    "Y_pred_test_nn = lr.predict(X_test)\n",
    "print(\"LOGICREGRESSION WITH C=5 \")\n",
    "print(confusion_matrix(Y_test, Y_pred_test_nn))\n",
    "print(f1_score(Y_test, Y_pred_test_nn))\n",
    "\n",
    "\n",
    "# Random Forest stories, mate!\n",
    "classif = RandomForestClassifier(n_estimators=100, n_jobs=2, min_samples_split=2, random_state=0)\n",
    "#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n",
    "#scores = cross_val_score(clf, X, y)\n",
    "#scores.mean()    \n",
    "classif.fit(x_train_res, y_train_res)\n",
    "Y_pred_test_rf = classif.predict(X_test)\n",
    "print(\"RANDOMFOREST\")\n",
    "print(confusion_matrix(Y_test, Y_pred_test_rf))\n",
    "print(f1_score(Y_test, Y_pred_test_rf))\n",
    "\n",
    "# Decision Tree, baby!\n",
    "classif2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\n",
    "classif2.fit(x_train_res, y_train_res)\n",
    "Y_pred_test_clf2 = classif2.predict(X_test)\n",
    "#scores = cross_val_score(clf, x_train, y_train)\n",
    "#scores.mean()\n",
    "print(\"DECISIONTREE\")\n",
    "print(confusion_matrix(Y_test, Y_pred_test_clf2))\n",
    "print(f1_score(Y_test, Y_pred_test_clf2))\n",
    "\n",
    "## Neural Network\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(x_train_res, y_train_res)\n",
    "y_pred_test_neural =clf.predict_proba(X_test) \n",
    "print(\"Neural Network\")\n",
    "print( y_pred_test_neural)\n",
    "#print(f1_score(Y_test, y_pred_test_neural))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "6246dba3-8506-4249-bebc-d03aa844f8c7",
    "_execution_state": "busy",
    "_uuid": "096fa0ef8326130a8a71c0d433cc19eb3c922a28",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic with Undersampling\n",
      "SENS  0.973214285714\n",
      "SPEC 0.94776119403\n",
      "ACC 0.959349593496\n",
      "Logistic with Oversampling\n",
      "SENS 1.0\n",
      "SPEC 1.0\n",
      "ACC 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Logistic/undersampling\n",
    "print (\"Logistic with Undersampling\")\n",
    "tn, fp, fn,tp = confusion_matrix(predictions,y_test).ravel() \n",
    "\n",
    "Sensitivity=tp/float((tp+fn))#Sensitivity \n",
    "print (\"SENS \",Sensitivity)\n",
    "\n",
    "Specificity=tn/float((tn+fp))#Specificity \n",
    "print (\"SPEC\",Specificity)\n",
    "\n",
    "Accuracy= accuracy_score(predictions,y_test, normalize=True, sample_weight=None)\n",
    "print (\"ACC\",Accuracy)\n",
    "\n",
    "## Logistic/Oversampling\n",
    "\n",
    "print (\"Logistic with Oversampling\")\n",
    "tn, fp, fn,tp = confusion_matrix(predictions2,Y_test).ravel() \n",
    "Sensitivity=tp/float((tp+fn))#Sensitivity \n",
    "print (\"SENS\",Sensitivity)\n",
    "\n",
    "Specificity=tn/float((tn+fp))#Specificity \n",
    "print (\"SPEC\",Specificity)\n",
    "\n",
    "Accuracy= accuracy_score(predictions2,Y_test, normalize=True, sample_weight=None)\n",
    "print (\"ACC\",Accuracy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
