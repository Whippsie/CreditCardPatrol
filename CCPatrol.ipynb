{
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "cells": [
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "822c18801d798c6491fe2017da165deb7456e06d",
        "_execution_state": "idle"
      },
      "source": "",
      "execution_count": null,
      "cell_type": "markdown",
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "dd7851674c1134ec916e56a1eb302ad297c9f84e",
        "_execution_state": "idle"
      },
      "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler # for preprocessing the data\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVC # SVM\nfrom sklearn.metrics import f1_score,confusion_matrix,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n#from sklearn.cross_validation import train_test_split # to split the data\n#from sklearn.cross_validation import KFold # For cross vbalidation\nfrom sklearn.model_selection import GridSearchCV # for tunnig hyper parameter it will use all combination of given parameters\nfrom sklearn.model_selection import RandomizedSearchCV # same for tunning hyper parameter but will use random combinations of parameters\nfrom sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n\n%matplotlib inline",
      "execution_count": 1,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "creditcard.csv\n\n"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "de2bb47c7032d4c409acca9e1f45f26a5305e4f3",
        "_execution_state": "idle"
      },
      "source": "data = pd.read_csv(\"../input/creditcard.csv\",header = 0)\ndata.head()",
      "execution_count": 2,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...         V21       V22       V23       V24  \\\n0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n\n        V25       V26       V27       V28  Amount  Class  \n0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]",
            "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "ffd654eccd998f4384cb56dce34ebbf27aacbb17",
        "_execution_state": "idle"
      },
      "source": "## Pre-processing the data\n## Normalizing the amount column\n\nfrom sklearn.preprocessing import StandardScaler\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\ndata = data.drop(['Time','Amount'],axis=1)\ndata.head()",
      "execution_count": 3,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "         V1        V2        V3        V4        V5        V6        V7  \\\n0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9       V10     ...           V21       V22       V23  \\\n0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n\n        V24       V25       V26       V27       V28  Class  normAmount  \n0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n\n[5 rows x 30 columns]",
            "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Class</th>\n      <th>normAmount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>0.090794</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>0</td>\n      <td>0.244964</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>-0.166974</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>0</td>\n      <td>-0.342475</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>0.207643</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>0</td>\n      <td>1.160686</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>-0.054952</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>0</td>\n      <td>0.140534</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>0.753074</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>0</td>\n      <td>-0.073403</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "24d39b64a751ee237c7c0ce371906503e1b8c020",
        "_execution_state": "idle"
      },
      "source": "## Since the data is largely imbalanced we need to resample the data such that the proportion/ratio between fraudulent and normal transactions are relativeley similar.\n\nx = data.loc[:, data.columns != 'Class']\ny = data.loc[:, data.columns == 'Class']",
      "execution_count": 4,
      "cell_type": "code",
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "8dd0e872c2e26996cccc20d4f3ab101335c37321",
        "_execution_state": "idle"
      },
      "source": "# Number of fraudelent transaction in the existing data\nnumberOffraudulentTransaction = len(data[data.Class == 1])\nfraudIndices = np.array(data[data.Class == 1].index)\n\n# Picking the indices of the normal classes\nnormalIndices = data[data.Class == 0].index\n\n# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\nrandom_normal_indices = np.random.choice(normalIndices, numberOffraudulentTransaction, replace = False)\nrandom_normal_indices = np.array(random_normal_indices)\n\n# Appending the 2 indices\nunder_sample_indices = np.concatenate([fraudIndices,random_normal_indices])\n\n# Under sample dataset\nunder_sample_data = data.iloc[under_sample_indices,:]\n\nx_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\ny_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']\n\n# Showing ratio\nprint(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\nprint(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\nprint(\"Total number of transactions in resampled data: \", len(under_sample_data))",
      "execution_count": 5,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Percentage of normal transactions:  0.5\nPercentage of fraud transactions:  0.5\nTotal number of transactions in resampled data:  984\n"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "df61e2342b27cf77acd8652f0ac0543010572e19",
        "_execution_state": "idle"
      },
      "source": "df = under_sample_data\n#train, validate, test = np.split(df.sample(frac=1), [int(.5*len(df)), int(.75*len(df))])\nx, x_test, y, y_test = train_test_split(x_undersample,y_undersample,test_size=0.25,train_size=0.75)\nx_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.33,train_size =0.66)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform (x_train)\nx_cv = scaler.fit_transform (x_cv)\nx_test = scaler.fit_transform (x_test)\n\n# cross-validate needs to be here (after the splitting for proper X-V)",
      "execution_count": 6,
      "cell_type": "code",
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "4676135f2593511796b62e34333ef531c5f53771",
        "_execution_state": "idle"
      },
      "source": "# My logic is regressing, guys!\nlogi = LogisticRegression(class_weight='balanced')\nmdl = logi.fit(x_train, y_train.values.ravel())\npredictions = logi.predict(x_test)\nplt.scatter(y_test, predictions)\nprint(\"LOGICREGRESS\")\nplt.xlabel(\"TruValues\")\nplt.ylabel(\"Predictions\")\nprint (mdl.score(x_test, y_test))\n\n# I will no put the receiver operating characteristic, no sir!\n\n# Support vector machine, boss!\n\nsvc = SVC(C=1, kernel='linear')\nsvc2 = SVC(C=1, kernel='polynomial')\nsvc3 = SVC(C=1, kernel='rbf')\nsvc4 = SVC(C=1, kernel='sigmoid')\nprint(\"SVM\")\n\n# Random Forest stories, mate!\nclassif = RandomForestClassifier(n_estimators=100, n_jobs=2, min_samples_split=2, random_state=0)\n#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n#scores = cross_val_score(clf, X, y)\n#scores.mean()    \nclassif.fit(x_train, y_train.values.ravel())\ny_pred_test_rf = classif.predict(x_test)\nprint(\"RANDOMFOREST\")\nprint(confusion_matrix(y_test, y_pred_test_rf))\nprint(f1_score(y_test, y_pred_test_rf))\n\n\n# Decision Tree, baby!\nclassif2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\nclassif2.fit(x_train, y_train.values.ravel())\ny_pred_test_clf2 = classif2.predict(x_test)\n#scores = cross_val_score(clf, x_train, y_train)\n#scores.mean()\nprint(\"DECISIONTREE\")\nprint(confusion_matrix(y_test, y_pred_test_clf2))\nprint(f1_score(y_test, y_pred_test_clf2))\n\n# Neural network, captain!\nlr = LogisticRegression(C = 1, penalty = 'l1')\nlr.fit(x_train, y_train.values.ravel())\ny_pred_test_nn = lr.predict(x_test)\nprint(\"NEURALNETWORK\")\nprint(confusion_matrix(y_test, y_pred_test_nn))\nprint(f1_score(y_test, y_pred_test_nn))\n\n# Extra Trees 4 social good, peepz!\nclassif3 = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n#scores = cross_val_score(clf, X, y)\n#scores.mean() > 0.999\n\n'''The main parameters to adjust when using these methods is n_estimators and max_features. \nThe larger n_estimators the better, but also the longer it  will take to compute. \nmax_feat is the size of the random subsets of features to consider when splitting a node. \nlower = greater the reduction of variance, but also the greater the increase in bias. \nEmpirical good default values are max_features=n_features for regression problems, \nand max_features=sqrt(n_features) for classification tasks (where n_features is the number of features \nin the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=1 \n(i.e., when fully developing the trees). The best parameter values should always be cross-validated. \nIn addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) \nwhile the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using \nbootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. \nThis can be enabled by setting oob_score=True.'''\n",
      "execution_count": 7,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "LOGICREGRESS\n0.943089430894\nSVM\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RANDOMFOREST\n[[120   4]\n [  8 114]]\n0.95\nDECISIONTREE\n[[113  11]\n [  9 113]]\n0.918699186992\nNEURALNETWORK\n[[118   6]\n [  7 115]]\n0.946502057613\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'The main parameters to adjust when using these methods is n_estimators and max_features. \\nThe larger n_estimators the better, but also the longer it  will take to compute. \\nmax_feat is the size of the random subsets of features to consider when splitting a node. \\nlower = greater the reduction of variance, but also the greater the increase in bias. \\nEmpirical good default values are max_features=n_features for regression problems, \\nand max_features=sqrt(n_features) for classification tasks (where n_features is the number of features \\nin the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=1 \\n(i.e., when fully developing the trees). The best parameter values should always be cross-validated. \\nIn addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) \\nwhile the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using \\nbootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. \\nThis can be enabled by setting oob_score=True.'"
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7fd3e011d630>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "transient": {}
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "096fa0ef8326130a8a71c0d433cc19eb3c922a28",
        "_execution_state": "idle"
      },
      "source": "tn, fp, fn,tp = confusion_matrix(predictions,y_test).ravel() \nSensitivity=tp/float((tp+fn))#Sensitivity \nprint (\"SENS\",Sensitivity)\n\nSpecificity=tn/float((tn+fp))#Specificity \nprint (\"SPEC\",Specificity)\n\nAccuracy= accuracy_score(predictions,y_test, normalize=True, sample_weight=None)\nprint (\"ACC\",Accuracy)",
      "execution_count": 10,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SENS 0.957627118644\nSPEC 0.9296875\nACC 0.943089430894\n"
        }
      ]
    }
  ]
}