{
  "nbformat": 4,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "execution_count": null,
      "outputs": [],
      "source": "",
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "822c18801d798c6491fe2017da165deb7456e06d",
        "_execution_state": "idle",
        "_cell_guid": "4306c3e3-093a-4316-a144-e07ee32079e9",
        "collapsed": false
      }
    },
    {
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "creditcard.csv\n\n"
        }
      ],
      "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVC # SVM\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import f1_score,confusion_matrix,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.model_selection import GridSearchCV # for tunnig hyper parameter it will use all combination of given parameters\nfrom sklearn.model_selection import RandomizedSearchCV # same for tunning hyper parameter but will use random combinations of parameters\nfrom sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n\n%matplotlib inline",
      "cell_type": "code",
      "metadata": {
        "_uuid": "dd7851674c1134ec916e56a1eb302ad297c9f84e",
        "_cell_guid": "66d8dc05-d7da-4cf9-8021-5a4a3505da33",
        "_execution_state": "idle",
        "collapsed": false,
        "trusted": false
      }
    },
    {
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...         V21       V22       V23       V24  \\\n0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n\n        V25       V26       V27       V28  Amount  Class  \n0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]",
            "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": "data = pd.read_csv(\"../input/creditcard.csv\",header = 0)\ndata.head()",
      "cell_type": "code",
      "metadata": {
        "_uuid": "de2bb47c7032d4c409acca9e1f45f26a5305e4f3",
        "_cell_guid": "d9b7d5fe-8192-4c5a-8b24-da0147140b74",
        "_execution_state": "idle",
        "collapsed": false,
        "trusted": false
      }
    },
    {
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "         V1        V2        V3        V4        V5        V6        V7  \\\n0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9       V10     ...           V21       V22       V23  \\\n0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n\n        V24       V25       V26       V27       V28  Class  normAmount  \n0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n\n[5 rows x 30 columns]",
            "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Class</th>\n      <th>normAmount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>0.090794</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>0</td>\n      <td>0.244964</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>-0.166974</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>0</td>\n      <td>-0.342475</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>0.207643</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>0</td>\n      <td>1.160686</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>-0.054952</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>0</td>\n      <td>0.140534</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>0.753074</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>0</td>\n      <td>-0.073403</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 30 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": "## Pre-processing the data\n## Normalizing the amount column\n\nfrom sklearn.preprocessing import StandardScaler\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\ndata = data.drop(['Time','Amount'],axis=1)\ndata.head()",
      "cell_type": "code",
      "metadata": {
        "_uuid": "ffd654eccd998f4384cb56dce34ebbf27aacbb17",
        "_cell_guid": "42130d29-9f0d-4248-bdb9-1514fcf2d151",
        "_execution_state": "idle",
        "collapsed": false,
        "trusted": false
      }
    },
    {
      "execution_count": 4,
      "outputs": [],
      "source": "## Since the data is largely imbalanced we need to resample the data such that the proportion/ratio between fraudulent and normal transactions are relativeley similar.\n\nx = data.loc[:, data.columns != 'Class']\ny = data.loc[:, data.columns == 'Class']",
      "cell_type": "code",
      "metadata": {
        "_uuid": "24d39b64a751ee237c7c0ce371906503e1b8c020",
        "_cell_guid": "d2405d3e-977a-4539-be51-aae0e4189a1d",
        "_execution_state": "idle",
        "collapsed": false,
        "trusted": false
      }
    },
    {
      "execution_count": 5,
      "outputs": [],
      "source": "#UNDERSAMPLING\n# Number of fraudelent transaction in the existing data\nnumberOffraudulentTransaction = len(data[data.Class == 1])\nfraudIndices = np.array(data[data.Class == 1].index)\n\n# Picking the indices of the normal classes\nnormalIndices = data[data.Class == 0].index\n\n# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\nrandom_normal_indices = np.random.choice(normalIndices, numberOffraudulentTransaction, replace = False)\nrandom_normal_indices = np.array(random_normal_indices)\n",
      "cell_type": "code",
      "metadata": {
        "_uuid": "8dd0e872c2e26996cccc20d4f3ab101335c37321",
        "_cell_guid": "1b49cbe6-8f76-48ca-8789-1ed231fcf18c",
        "_execution_state": "idle",
        "collapsed": false,
        "trusted": false
      }
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "9d616ab4b30552e9ec4f8531005f66210c9e715e",
        "_execution_state": "idle"
      },
      "source": "#UNDERSAMPLING\n# Appending the 2 indices\nunder_sample_indices = np.concatenate([fraudIndices,random_normal_indices])\n\n# Under sample dataset\nunder_sample_data = data.iloc[under_sample_indices,:]\n\nx_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\ny_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']\n\n# Showing ratio\nprint(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\nprint(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\nprint(\"Total number of transactions in resampled data: \", len(under_sample_data))",
      "execution_count": 6,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Percentage of normal transactions:  0.5\nPercentage of fraud transactions:  0.5\nTotal number of transactions in resampled data:  984\n"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "d07eca2a35bbe3027e1d2576965d95f9f16f8efd",
        "_execution_state": "idle"
      },
      "source": "#OVERSAMPLING\n## Splitting the data into Training,Validation and Test Set##\n## Test Set needs to be unused till the mere end##\nX_train, X_test, Y_train, Y_test = train_test_split(data,y, test_size=0.25, random_state=42)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size=0.25, random_state=42)\n# #Figuring out the ratio of normal transction and fraudelent transaction from training data# #\nnormal_tdata = X_train[X_train[\"Class\"]==0]\nprint(\"train data: length of normal data\",len(normal_tdata))\nfraud_tdata = X_train[X_train[\"Class\"]==1]\nprint(\"train data: length of fraud data\",len(fraud_tdata))\n## dataset for validation set ##\nnormal_vdata = X_val[X_val[\"Class\"]==0]\nprint(\"For Validation Set :length of normal data\",len(normal_vdata))\nfraud_vdata = X_val[X_val[\"Class\"]==1]\nprint(\"For Validation Set :length of fraud data\",len(fraud_vdata))",
      "execution_count": 7,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "train data: length of normal data 159920\ntrain data: length of fraud data 283\nFor Validation Set :length of normal data 53306\nFor Validation Set :length of fraud data 96\n"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "b927676a28fb48050cb6d63d0382774618b2075d",
        "_execution_state": "idle"
      },
      "source": "#SMOTE\n#Since the data is highly imbalanced we use the sklearn package to balance out the data by introducing more fraudulent data ##\n#basically oversampling of data \nsm = SMOTE(random_state=12, ratio = 'auto', k_neighbors=5)\n#Possible ratios : minority, majority, not minority, all, auto\nx_train_res, y_train_res = sm.fit_sample(X_train, Y_train.values.ravel())",
      "execution_count": 8,
      "cell_type": "code",
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "_uuid": "4bd0bd557bc3e7f8713e13ab981a8d5e2ab11309",
        "_execution_state": "idle"
      },
      "source": "a = x_train_res[:,28]\nb= np.count_nonzero(a == 1)\nc= np.count_nonzero(a == 0)\nprint(\"length of oversampled data is \",len(x_train_res))\nprint(\"Number of normal transcation in oversampled data\",b)\nprint(\"No.of fraud transcation\",c)\nprint(\"Proportion of Normal data in oversampled data is \",c/len(x_train_res))\nprint(\"Proportion of fraud data in oversampled data is \",b/len(x_train_res))",
      "execution_count": 9,
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "length of oversampled data is  319840\nNumber of normal transcation in oversampled data 159920\nNo.of fraud transcation 159920\nProportion of Normal data in oversampled data is  0.5\nProportion of fraud data in oversampled data is  0.5\n"
        }
      ]
    },
    {
      "execution_count": 10,
      "outputs": [],
      "source": "print (\"UNDERSAMPLING\")\ndf = under_sample_data\n#train, validate, test = np.split(df.sample(frac=1), [int(.5*len(df)), int(.75*len(df))])\nx, x_test, y, y_test = train_test_split(x_undersample,y_undersample,test_size=0.25,train_size=0.75)\nx_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.33,train_size =0.66)\n\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform (x_train)\nx_cv = scaler.fit_transform (x_cv)\nx_test = scaler.fit_transform (x_test)\n\n# cross-validate needs to be here (after the splitting for proper X-V)",
      "cell_type": "code",
      "metadata": {
        "_uuid": "df61e2342b27cf77acd8652f0ac0543010572e19",
        "_cell_guid": "21214f06-a021-46a4-bbfb-132e020c0f1e",
        "_execution_state": "idle",
        "collapsed": false,
        "trusted": false
      }
    },
    {
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "UNDERSAMPLING\n0.930894308943\n[[125   0]\n [ 17 104]]\n             precision    recall  f1-score   support\n\n          0       0.88      1.00      0.94       125\n          1       1.00      0.86      0.92       121\n\navg / total       0.94      0.93      0.93       246\n\nLOGICREGRESS\n0.930894308943\nSVM\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RANDOMFOREST\n[[124   1]\n [ 19 102]]\n0.910714285714\nDECISIONTREE\n[[120   5]\n [ 15 106]]\n0.913793103448\nNEURALNETWORK\n[[125   0]\n [ 18 103]]\n0.919642857143\nOVERSAMPLING\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1.0\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[71089     0]\n [    0   113]]\n             precision    recall  f1-score   support\n\n          0       1.00      1.00      1.00     71089\n          1       1.00      1.00      1.00       113\n\navg / total       1.00      1.00      1.00     71202\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "LOGICREGRESS2\n1.0\nSVM\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RANDOMFOREST\n[[71089     0]\n [    0   113]]\n1.0\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DECISIONTREE\n[[71089     0]\n [    0   113]]\n1.0\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "NEURALNETWORK\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[71089     0]\n [    0   113]]\n1.0\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'The main parameters to adjust when using these methods is n_estimators and max_features. \\nThe larger n_estimators the better, but also the longer it  will take to compute. \\nmax_feat is the size of the random subsets of features to consider when splitting a node. \\nlower = greater the reduction of variance, but also the greater the increase in bias. \\nEmpirical good default values are max_features=n_features for regression problems, \\nand max_features=sqrt(n_features) for classification tasks (where n_features is the number of features \\nin the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=1 \\n(i.e., when fully developing the trees). The best parameter values should always be cross-validated. \\nIn addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) \\nwhile the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using \\nbootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. \\nThis can be enabled by setting oob_score=True.'"
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7f9c38b53ba8>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQZLU/H/3fcABImD48QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "transient": {}
        }
      ],
      "source": "#UNDERSAMPLING\n# My logic is regressing, guys!\nlogi = LogisticRegression(class_weight='balanced')\nmdl = logi.fit(x_train, y_train.values.ravel())\npredictions = logi.predict(x_test)\nprint(accuracy_score(y_test, predictions))\nprint (confusion_matrix(y_test, predictions))\nprint (classification_report(y_test, predictions))\nplt.scatter(y_test, predictions)\nprint(\"LOGICREGRESS\")\nplt.xlabel(\"TruValues\")\nplt.ylabel(\"Predictions\")\nprint (mdl.score(x_test, y_test))\n\n\n# I will not put the receiver operating characteristic, no sir!\n\n# Support vector machine, boss!\n\nsvc = SVC(C=1, kernel='linear')\nsvc2 = SVC(C=1, kernel='polynomial')\nsvc3 = SVC(C=1, kernel='rbf')\nsvc4 = SVC(C=1, kernel='sigmoid')\nprint(\"SVM\")\n\n# Random Forest stories, mate!\n\nclassif = RandomForestClassifier(n_estimators=100, n_jobs=2, min_samples_split=2, random_state=0)\n#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n#scores = cross_val_score(clf, X, y)\n#scores.mean()    \nclassif.fit(x_train, y_train.values.ravel())\ny_pred_test_rf = classif.predict(x_test)\nprint(\"RANDOMFOREST\")\nprint(confusion_matrix(y_test, y_pred_test_rf))\nprint(f1_score(y_test, y_pred_test_rf))\n\n\n# Decision Tree, baby!\nclassif2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\nclassif2.fit(x_train, y_train.values.ravel())\ny_pred_test_clf2 = classif2.predict(x_test)\n#scores = cross_val_score(clf, x_train, y_train)\n#scores.mean()\nprint(\"DECISIONTREE\")\nprint(confusion_matrix(y_test, y_pred_test_clf2))\nprint(f1_score(y_test, y_pred_test_clf2))\n\n# Neural network, captain!\nlr = LogisticRegression(C = 1, penalty = 'l1')\nlr.fit(x_train, y_train.values.ravel())\ny_pred_test_nn = lr.predict(x_test)\nprint(\"NEURALNETWORK\")\nprint(confusion_matrix(y_test, y_pred_test_nn))\nprint(f1_score(y_test, y_pred_test_nn))\n\n# Extra Trees 4 social good, peepz!\nclassif3 = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n#scores = cross_val_score(clf, X, y)\n#scores.mean() > 0.999\n\n\nprint (\"OVERSAMPLING\")\n\nlogi = LogisticRegression(class_weight='balanced')\nmdl = logi.fit(X_train, Y_train.values.ravel())\npredictions2 = logi.predict(X_test)\nprint(accuracy_score(Y_test, predictions2))\nprint (confusion_matrix(Y_test, predictions2))\nprint (classification_report(Y_test, predictions2))\nplt.scatter(Y_test, predictions2)\nprint(\"LOGICREGRESS2\")\nplt.xlabel(\"TruValues\")\nplt.ylabel(\"Predictions\")\nprint (mdl.score(X_test, Y_test))\n\nsvc = SVC(C=1, kernel='linear')\nsvc2 = SVC(C=1, kernel='polynomial')\nsvc3 = SVC(C=1, kernel='rbf')\nsvc4 = SVC(C=1, kernel='sigmoid')\nprint(\"SVM\")\n\n# Random Forest stories, mate!\n\nclassif = RandomForestClassifier(n_estimators=100, n_jobs=2, min_samples_split=2, random_state=0)\n#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n#scores = cross_val_score(clf, X, y)\n#scores.mean()    \nclassif.fit(X_train, Y_train.values.ravel())\nY_pred_test_rf = classif.predict(X_test)\nprint(\"RANDOMFOREST\")\nprint(confusion_matrix(Y_test, Y_pred_test_rf))\nprint(f1_score(Y_test, Y_pred_test_rf))\n\n\n# Decision Tree, baby!\nclassif2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\nclassif2.fit(X_train, Y_train.values.ravel())\nY_pred_test_clf2 = classif2.predict(X_test)\n#scores = cross_val_score(clf, x_train, y_train)\n#scores.mean()\nprint(\"DECISIONTREE\")\nprint(confusion_matrix(Y_test, Y_pred_test_clf2))\nprint(f1_score(Y_test, Y_pred_test_clf2))\n\n# Neural network, captain!\nlr = LogisticRegression(C = 1, penalty = 'l1')\nlr.fit(X_train, Y_train.values.ravel())\nY_pred_test_nn = lr.predict(X_test)\nprint(\"NEURALNETWORK\")\nprint(confusion_matrix(Y_test, Y_pred_test_nn))\nprint(f1_score(Y_test, Y_pred_test_nn))\n\n# Extra Trees 4 social good, peepz!\nclassif3 = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n#scores = cross_val_score(clf, X, y)\n#scores.mean() > 0.999\n'''The main parameters to adjust when using these methods is n_estimators and max_features. \nThe larger n_estimators the better, but also the longer it  will take to compute. \nmax_feat is the size of the random subsets of features to consider when splitting a node. \nlower = greater the reduction of variance, but also the greater the increase in bias. \nEmpirical good default values are max_features=n_features for regression problems, \nand max_features=sqrt(n_features) for classification tasks (where n_features is the number of features \nin the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=1 \n(i.e., when fully developing the trees). The best parameter values should always be cross-validated. \nIn addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) \nwhile the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using \nbootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. \nThis can be enabled by setting oob_score=True.'''\n",
      "cell_type": "code",
      "metadata": {
        "_uuid": "4676135f2593511796b62e34333ef531c5f53771",
        "_cell_guid": "6d6a5d38-7a63-4112-a5f8-bfc8d9467bd7",
        "_execution_state": "idle",
        "collapsed": false,
        "trusted": false
      }
    },
    {
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SENS 1.0\nSPEC 0.880281690141\nACC 0.930894308943\nSENS 1.0\nSPEC 1.0\nACC 1.0\n"
        }
      ],
      "source": "tn, fp, fn,tp = confusion_matrix(predictions,y_test).ravel() \nSensitivity=tp/float((tp+fn))#Sensitivity \nprint (\"SENS\",Sensitivity)\n\nSpecificity=tn/float((tn+fp))#Specificity \nprint (\"SPEC\",Specificity)\n\nAccuracy= accuracy_score(predictions,y_test, normalize=True, sample_weight=None)\nprint (\"ACC\",Accuracy)\n\ntn, fp, fn,tp = confusion_matrix(predictions2,Y_test).ravel() \nSensitivity=tp/float((tp+fn))#Sensitivity \nprint (\"SENS\",Sensitivity)\n\nSpecificity=tn/float((tn+fp))#Specificity \nprint (\"SPEC\",Specificity)\n\nAccuracy= accuracy_score(predictions2,Y_test, normalize=True, sample_weight=None)\nprint (\"ACC\",Accuracy)",
      "cell_type": "code",
      "metadata": {
        "_uuid": "096fa0ef8326130a8a71c0d433cc19eb3c922a28",
        "_cell_guid": "6246dba3-8506-4249-bebc-d03aa844f8c7",
        "_execution_state": "idle",
        "collapsed": false,
        "trusted": false
      }
    }
  ],
  "nbformat_minor": 0
}